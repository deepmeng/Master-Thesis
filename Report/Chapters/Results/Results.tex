\chapter{Results}
For monitoring and understanding better the training process, some performance and loss measures were taken into account in generating graphs. 

The relevant performance measures are the length of the episode, accumulated reward, and the value function result. These values should be growing. The length of the episode grows until the agent has been trained to drive the whole lap. The more the car stays in the game and the better it drives, the more reward it gets and the better value function results. If the car crashes then the length of the episode is short, and because it is learning, the graph should show an increasing curve as the training advances. 

The loss measures are the ones that participate in the composition of the loss function. These include policy, value, and entropy losses. These results should show decreasing curves as the loss is being minimized with each step and the estimations become more accurate. The entropy is representing the amount of exploration that the agent is making, the more it learns, the less it explores, and acts more based on his knowledge.

The following figure presents the results of training an agent using A3C: 4 workers in parallel, the Adam optimizer with the learning rate of $10^{-4}$.
\begin{figure}[H]
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{Figures/Length}
	\caption{Length}\label{fig:Length}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{Figures/Reward}
	\caption{Reward}\label{fig:Reward}
	\endminipage
\end{figure}
\begin{figure}[H]
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{Figures/EntropyLoss}
	\caption{Entropy Loss}\label{fig:EntropyLoss}
	\endminipage\hfill
	\minipage{0.5\textwidth}
	\includegraphics[width=\linewidth]{Figures/Loss}
	\caption{Loss}\label{fig:Loss}
	\endminipage
\end{figure}

As we can see from the figures the training has finished when the agent learned to drive the whole lap and the monotonicity of the plotted curves are as expected (as described above). After $5-6$ hours ($300$ steps) of training the vales started to converge. The trained agent was played on different tracks to test that it learned to drive, but more about this in the section about the different tracks.

The chapter continues with presenting the different scenarios that offer insights in performance and environment manipulations. More precisely, the performance of two different optimizers will be compared, the scenarios with different number of workers will be presented, and different action spaces. The environment specific experiments list some manipulations with the reward function, acceleration and playing the trained agent on other tracks, too.

%network
\input{Chapters/Results/Optimizers}
\input{Chapters/Results/Multithreading}
\input{Chapters/Results/ContinuousvsDiscreteActionSpace}
%environment
\input{Chapters/Results/RewardFunction}
\input{Chapters/Results/Acceleration}
\input{Chapters/Results/Tracks}