\section{Reward}\label{sectionReward}
The reward is really important in reinforcement learning, because it is the one that the agent has to maximize for learning the agent how to act in the environment. For example, consider teaching a dog a new trick: you cannot tell it what to do, but you can reward/punish it if it does the right/wrong thing. It has to figure out what it did that made it get the reward/punishment, which is known as the credit assignment problem \cite{reward_small}. We can use a similar method to train the agent how to drive a car. More about the reward can be read in \Cref{TheorBkgdRL}. 
	
In this project two reward function is compared, for learning the agent how to drive a car in the TORCS environment.  

\subsection*{First reward function}
The first reward function used in this project is the reward function from the paper \cite{DBLP:journals/corr/LillicrapHPHETS15}: \\
\textit{"For the Torcs environment we used a reward function which provides a positive reward each step for the velocity of the car projected along the direction and a penalty of -1 for collisions. Episodes were terminated if progress was not made along the track after 500 frames."}\\
To understand this a figure to illustrate the reward can be seen on \Cref{fig:Reward_paper}.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{Figures/Result/Reward_paper.pdf}
	\caption{Explanation of the reward used in the paper \cite{DBLP:journals/corr/LillicrapHPHETS15} }
	\label{fig:Reward_paper}
\end{figure}

The reward function is then:
\begin{equation}
Reward = V_{car} \cdot cos(\theta) 
\end{equation}
The idea this reward function uses are getting an reward for how fast the car drives in the center of the track. The agent gets more reward, when the car is driving fast at the center of the track, and less reward when the car is driving fast away from the center of the track. 

\subsection*{Second reward function}
After using the first reward function it was concluded that this reward function didn't perform as expected. Because the reward function is important for learning, another reward function is tried. This reward function is taken from the blog \cite{DDPG_Torcs}. Here it uses the same idea, that the reward should be bigger if the car drives fast in the center of the track, and less reward when it is far away from the center. The reward function looks like this:

\begin{equation}
Reward = V_{car} \cdot cos(\theta) - V_{car} \cdot sin(\theta) - V_{car} \cdot \mid trackPos\mid 
\end{equation}
The TrackPos gives percent the car is off center of track. Which is useful in this reward function, when it is wished the car is near the center.  

In this equation the first term want to maximum longitudinal velocity, second term try to minimize transverse velocity, and then it also penalize the AI if it constantly drives very off center of the track in the third term.

Here we see the agent learns how to drive, and this is the reward function where the best results came in this project. It improves the stability of the training.

To test the two reward functions influence on the agent learning, all other parameters stays as described in the introduction to this \Cref{cha:Result}. Thereby it should be possible to only see the influence of the two different reward functions.

To compare both reward functions a graph is created, this graph can be seen on \Cref{fig:change_of_Reward_reward_graph}:

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{Figures/Result/change_of_Reward_reward_graph.pdf}
	\caption{Comparison of the two different reward function with the reward getting from the environment}
	\label{fig:change_of_Reward_reward_graph}
\end{figure}

The \Cref{fig:change_of_Reward_reward_graph} shows the mean total reward of 5 time-steps (episodes). The reward after every step is added together to get the total reward. The total reward is then the reward when an episode is finish. An episode is finished when the car is out of track, the car is driving backwards or the car has finished a lap on the track.  

On the graph on \Cref{fig:change_of_Reward_reward_graph} it is seen that both reward function have the wanted effect, and the first reward function learns a bit faster than the second reward function around 250 time-steps and the second reward function learn around 320 time-steps. The negative side of the first reward function is it is not as stable as the second reward function, by looking at the graphs the second reward function is more stable than the first. 

The stability of the learning can also be seen on the car in the TORCS environment. Here it looks like the first reward function drives close to the edge of the track, and learns how to follow the red and white stripes edges. In the second reward function the car learns to drive better near the center of the road, where the car is more following the white lines in the center of the road. 

The second reward function works best, it is then the reward function which has been used in the final algorithm for the agent.  
