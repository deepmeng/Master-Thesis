\subsubsection*{Convolutional Neural Networks (CNNs)}

Convolutional Neural Networks are similar to normal Neural Networks. They are made up of neurons that have learn able weights and biases. Each neuron receives some inputs, perform a dot product and sometimes follows it with a non-linearity. The whole network expresses a single differentiable score function - from raw image pixels to class scores. CNN architectures make the assumption that the inputs is images, which make it possible to encode certain properties into the architecture. It makes the forward function more efficient to implement and reduce the amount of parameters in the network. \cite{CNN_course}      

The problem about regular neural networks is it doesn't scale well to full images. An example is the CIFAR-10 \cite{CIFAR_10}, Here are the images only of size 32x32x3 (32 wide, 32 high, 3 color channels). A single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32*32*3 = 3072 weights. For images in bigger sizes, e.g. 200x200x3, would lead to neurons that have 200*200*3 = 120,000 weights. This full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.

Convolutional neural networks take advantages of the fact that the input consist of images. It is done by instead of in regular neural networks, the layers of a CNN have neurons arranged in 3 dimensions: width, height, depth. Example of the CIFAR-10 are an input volume of activations, an the volume has dimensions 32x32x3 (width, height, depth respectively).The neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. To see this different we compare \Cref{fig:feedforward} with the figure below \Cref{fig:NN_vs_CNN}.  

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{Figures/NN_vs_CNN.pdf}
	\caption{A Convolutional Neural Network arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers. Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be 3 (Red, Green, Blue channels) \cite{CNN_course}}
	\label{fig:NN_vs_CNN}
\end{figure}

\subsubsection*{Layers}
As described earlier a CNN is a combination of layers and every layer transforms one volume of activations to another  through a differentiable function. CNN uses three main types of layers to build the architecture: Convolutional Layer, Pooling Layer, and Fully-Connected Layer (exactly as seen in regular Neural Networks). We will stack these layers to form a full CNN architecture. In reinforcement learning is the pooling layer not used, because they buy translation invariance - the network becomes insensitive to the location of an object in the image.

\subsubsection*{Convolutional Layer}
The Convolutional layer is the main building block of a Convolutional Network that does most of the computational heavy lifting. The convolutional layer consist of a set of learnable filters. every filter is small spatially (along width and height), but extend through the full depth of the input volume. An example of the first layer in a CNN is a filter with size 5x5x3. During the first forward pass it is slide/convolved each filter across the height and width of the input volume and compute dot products between the entries of the filter and the input at any position. As the filter is slide over the input volume it produces a 2-dimensional activation map. The activation map shows the responses of that filter at all spatial position. The network will learn filters that activate when they sees some type of visual feature - such as an edge of some orientation or a patch of some colors. On higher layers the network will learn to see honeycomb or wheel-like patters, so more complete figures. On each convolutional layer it will have an entire set of filters, each layer will produce a separate 2-dimensional activation map. The activation maps will be stacked along the depth dimension and produce the output volume. 

When dealing with high dimensional inputs like images, it is impractical to connect neurons to all neurons in the previous volume. Instead it is smart to connect each neuron to only a local region of the input volume. the spatial extent of this connectivity is a hyperparameter  called the receptive field of the neuron - equivalently this is the filter size. An illustration of the receptive can be seen on \Cref{fig:Respective_field}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{Figures/Respective_field.pdf}
	\caption{An example input volume in red (e.g. a 32x32x3 CIFAR-10 image), and an example volume of neurons in the first Convolutional layer. Each neuron in the convolutional layer is connected only to a local region in the input volume spatially, but to the full depth (i.e. all color channels). Note, there are multiple neurons (5 in this example) along the depth, all looking at the same region in the input \cite{CNN_course}}
	\label{fig:Respective_field}
\end{figure}

\textbf{Spatial arrangement}
After describing the connectivity of each neuron in the convolutional layer to the input volume, this is done by describing the spatial arrangement. Spatial arrangement include how many neurons there are in the output volume and how they are arranged. Three hyperparameters control the size of the output volume - the depth, stride and zero-padding. 

The first hyperparameter is the depth, it correspond to how many filters the convolutional layer use. Each filter looking for something different in the input. An example is the convolutional layer takes a raw image as an input, then different neurons along the depth dimension may activate in presence of various oriented edges or blobs of colors. A set of neurons that are all looking at the same region of the input is called \textit{depth colum}n or \textit{fibre}

Another hyperparameter is the stride - it defines the stride the filters is slide over the input. If the stride is 1 the filters move one pixel at a time. When the stride is 2 then the filters jump 2 pixels at a time as the filters is slided around. This will produce smaller output volumes spatially.

The last hyperparameter to control the size of the output volume is the size of zero-padding. Zero-padding pad the input volume with zeros around the border. The good feature with zero-padding is it control the spatial size of the output volumes. This is useful to preserve the spatial size of the input volume so the input and output width and height are the same. 

The way to compute the spatial size of the output volume as a function of the input volume size ($\textbf{W}$), the receptive field size of the convolutional layer neurons($\textbf{F}$), the stride with which they are applied ($\textbf{S}$) and the amount of zero padding used ($\textbf{P}$) on the border. The formula for calculating how many neurons "fit" is:
\begin{equation}
\frac{W-F+2P}{S}+1
\end{equation}    
An example for a 5x5 input and a 3x3 filter with stride 1 and zero-padding 1 the output would be of the spatial size 5x5:
\begin{equation}
\frac{5-3+2\cdot1 }{1}+1         \rightarrow             \frac{4}{1}+1 =5
\end{equation} 
And with stride 2 the output would be 3x3:
\begin{equation}
\frac{7-3+2\cdot1 }{2}+1         \rightarrow             \frac{4}{2}+1 =3
\end{equation} 
The visualization can be seen on the figure below \Cref{fig:Spatial_size}: 

\begin{figure}[H]
	\centering
	\includegraphics[width=1\textwidth]{Figures/Spatial_size.pdf}
	\caption{Illustration of spatial arrangement. The example is described above. In this example there is only one spatial dimension (x-axis), one neuron with a receptive field size of F = 3, the input size is W = 5, and there is zero padding of P = 1. \textbf{Left:} The neuron strided across the input in stride of S = 1. \textbf{Right:} The neuron uses stride of S = 2
	The neuron weights are in this example [1,0,-1] (shown on very right), and its bias is zero. These weights are shared across all yellow neurons. \cite{CNN_course}}
	\label{fig:Spatial_size}
\end{figure} 

\subsubsection*{Summary of convolutional layers}
To summarize the convolutional layer
\begin{itemize}
	\item Accept a input volume of size $W_1 \times  H_1 \times  D_1$
	\item Requires four hyperparameters:
	\begin{itemize}
		\item Number of filters $K$
		\item The receptive field size of the Convolutional Layer neurons $F$ 
		\item The stride $S$
		\item The amount of zero-padding $P$ 
	\end{itemize}
	\item Output a volume of size $W_2 \times  H_2 \times  D_2$
	\begin{itemize}
		\item $W_2 = \frac{W_1-F+2P}{S}+1$
		\item $H_2 = \frac{H_1-F+2P}{S}+1$
		\item $D_2 = K$
	\end{itemize}
	\item In the output volume the $d$-th depth slide (of side $W_2 \times  H_2$) s the result of performing a valid convolution of the $d$-th filter over the input volume with a stride of $S$, and then offset by $d$-th bias
\end{itemize}

\subsubsection*{Fully connected Layer }
The fully connected layer is a traditional Multi Layer Perceptron. The term “Fully Connected” implies that every neuron in the previous layer is connected to every neuron on the next layer. The output from the convolutional layers represent a high-level features of the input image. The purpose of the Fully Connected layer is to use these features for classifying the input image into various classes. 

Apart from classification fully connected layer is also a cheap way to learn non-linear features from these layers. By combining these features, the classification of the network would be even better. \cite{Fully_Connected_Layer}         