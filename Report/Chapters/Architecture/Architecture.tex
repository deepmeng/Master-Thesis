\chapter{Methods}
\label{chap:projectdef}
This chapter will explain the methods implemented in this project. The methods used are state-of-the-art methods in deep reinforcement learning, that are decided based on their impact in the area. One of the companies which is ahead of time in developing new reinforcement learning methods is DeepMind. All the methods in this project are methods published by DeepMind in the recent years. A figure showing an overview of the methods used in this project can be seen on \Cref{fig:Methods_deepmind}     

\begin{figure}[H]
	\centering
	\includegraphics[width=1.25\textwidth]{Figures/Architecture/Methods_deepmind.pdf}
	\caption{Timeline of methods published by DeepMind. Methods explain and implemented in this chapter is in a red box.   }
	\label{fig:Methods_deepmind}
\end{figure}

All the methods shown on \Cref{fig:Methods_deepmind} are methods found on DeepMind's website \cite{Publications_Deepmind}. The first method used and explained is the DQN (Deep Q-Network) from the paper \cite{DBLP:journals/corr/MnihKSGAWR13}. A short description of this method is also found in \Cref{Previous_Research}, and it had a big impact on deep reinforcement learning.

The next method this project has worked with is the DDPG (Deep Deterministic Policy Gradient). This method is the first method used for continuous control. The method is from the paper \cite{DBLP:journals/corr/LillicrapHPHETS15}. 

By looking at the timeline, the next two algorithms by DeepMind are not implemented and explained. The DDQN (Double Deep Q-Network) is a more advanced version of the DQN and can be found in the paper \cite{DBLP:journals/corr/HasseltGS15}. The other method is the one DeepMind used to beat Lee Sedol in the game of Go. A short description of the paper can be found in \Cref{Previous_Research}. An explanation of the Method DeepMind used in the game of Go can be found in the paper \cite{Silver_2016}. 

The last method explained and implemented in this project is the A3C (Asynchronous Advantage Actor Critic) method. This is a method which uses different agents to train in the environment. A3C is the method which has been used mostly in this project, because it is the method which performed best according to the paper \cite{DBLP:journals/corr/MnihBMGLHSK16} where it was published by DeepMind.

All the method are implemented and tested in this project in order to explore the differences and get a knowledge of how they worked. The DDPG and A3C is implemented in the TORCS environment, which is also explained in this chapter.




%\begin{figure}[H]
%	\centering
% \includegraphics[width=0.3\textwidth]{Figures/ProjectFramework/Project_framework_diagram}
%	\caption{The framework for the system used in this project}
%	\label{fig:Project_framework}
%\end{figure}

\input{Chapters/Architecture/DQN}
\input{Chapters/Architecture/DDPG}
\input{Chapters/Architecture/TORCS}
\input{Chapters/Architecture/A3C}