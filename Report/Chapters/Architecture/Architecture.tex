\chapter{Methods}
\label{chap:projectdef}
This chapter will explain the methods implemented in this project. The methods used is state of the art methods in deep reinforcement learning, they are decided from their impact in the area. One of the companies which is in front of developing new reinforcement learning methods is DeepMind. All the methods in this project is method published by DeepMind in the recent years. A figure to show an overview of the methods used in this project, can be seen on \Cref{fig:Methods_deepmind}     

\begin{figure}[H]
	\centering
	\includegraphics[width=1.25\textwidth]{Figures/Architecture/Methods_deepmind.pdf}
	\caption{Timeline of methods published by DeepMind. Methods explain and implemented in this chapter is in a red box.   }
	\label{fig:Methods_deepmind}
\end{figure}

All the methods shown on \Cref{fig:Methods_deepmind} is method found on DeepMind's website \cite{Publications_Deepmind}. The first method used and explained is the DQN (Deep Q-Network) from the paper \cite{DBLP:journals/corr/MnihKSGAWR13}. A short description of this method is also found in \Cref{Previous_Research}, and had a big impact on deep reinforcement learning.    

The next method this project has worked with is the DDPG (Deep Deterministic Policy Gradient) this method is the first method used for continuous control. The method is from the paper \cite{DBLP:journals/corr/LillicrapHPHETS15}. 

By looking at the timeline the next two algorithm by DeepMind is not implemented and explained. The DDQN (Double Deep Q-Network) is a more advance version of the DQN and can be found in the paper \cite{DBLP:journals/corr/HasseltGS15}. The other method is the one DeepMind used to beat Lee Sedol in the game Go, a short description of the paper can be found in \Cref{Previous_Research}. An explanation of the Method DeepMind used in the game of Go, can be found in the paper \cite{Silver_2016}. 

The last method explained and implemented in this project is the A3C (Asynchronous Advantage Actor Critic) method, this is a method which uses different agent to train in the environment. A3C is the method which has been used mostly in this project, because it is the method which performed best. The method was published by DeepMind in the paper \cite{DBLP:journals/corr/MnihBMGLHSK16}.  

All the method is implemented and tested in this project to explore the differences and get a knowledge of how they worked. The DDPG and A3C is implemented in the TORCS environment, which is also explained in this chapter. 




%\begin{figure}[H]
%	\centering
% \includegraphics[width=0.3\textwidth]{Figures/ProjectFramework/Project_framework_diagram}
%	\caption{The framework for the system used in this project}
%	\label{fig:Project_framework}
%\end{figure}

\input{Chapters/Architecture/DQN}
\input{Chapters/Architecture/DDPG}
\input{Chapters/Architecture/TORCS}
\input{Chapters/Architecture/A3C}