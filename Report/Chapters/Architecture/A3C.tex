\section{Asynchronous Advantage Actor Critic}
The newest breakthrough in RL is the \textit{Asynchronous Advantage Actor Critic} (A3C) approach, and therefore it was chosen to be studied and implemented onto the idea of driver-less cars. In order to achieve a similar environment as the one of a car, and also to be able to get the necessary data easier a car simulator was chosen for the project. Among the existing car simulators encountered in the different online sources, and after analyzing the DDPG implementation on The Open Racing Car Simulator (TORCS) \cite{DDPG_Torcs}, the project settled for this car simulator as well.

The project was mainly inspired from the article \cite{DBLP:journals/corr/MnihBMGLHSK16} summarized in the section \ref{AsyncMeths}, and also from the recent implementation of the A3C into the Doom game elaborated on in \cite{A3CDoom}.

The idea behind the A3C is very much around the same \textit{actor-critic} approach described in the section \ref{PolicyGradMeths}, that more accurately is founded on the presence of both the value function approximator, $\theta_{v}$ and the bootstrapping policy estimator, $\theta$. An additional feature to the actor-critic method is the \textit{asynchronous} part. Instead of having a single agent training on the GPU, multiple agents are instantiated for training on different CPU threads simultaneously, and they share a global network, which is updated as the agents advance. Another new feature of the A3C is the \textit{advantage} element, which is just a mathematical way of expressing how much better some actions ended up to be, and where the estimation should be improved. The update performed by the A3C is of the form $\nabla_{\theta'}\textup{log}\pi(a|s,\theta')A(s,a,\theta,\theta_{v})$, and the formula for the advantage is $\sum_{i=0}^{k-1}\gamma^{i}r_{t+i}+\gamma^kV(s_{t+k},\theta_{v})-V(s_{t},\theta_{v})$, which are both taken from the article \cite{DBLP:journals/corr/MnihBMGLHSK16}. The update formula changes slightly after including the entropy factor in the policy in order to encourage exploration and avoid convergence to an earlier suboptimal solution. The detailed A3C algorithm taken from \cite{DBLP:journals/corr/MnihBMGLHSK16} is listed below.
\begin{algorithm}[H]
	\caption{Asynchronous advantage actor-critic - pseudocode for each actor-learner thread.}
	\label{algo:A3C}
	\begin{algorithmic}
		\State \textit{//Assume global shared parameter vectors $\theta$ and $\theta_{v}$ and counter $T=0$}
		\State \textit{//Assume thread-specific parameter vectors $\theta'$ and $\theta_{v}'$}
		\State Initialize thread step counter $t\leftarrow1$
		\Repeat
		\State Reset gradients: $d\theta\leftarrow 0$ and $d\theta_{v}\leftarrow0$
		\State Synchronize thread-specific parameters $\theta'=\theta$ and $\theta_{v}'=\theta_{v}$
		\State $t_{start}=t$
		\State Get state $s_{t}$
		\Repeat
		\State Perform action $a_{t}$ according to policy $\pi(a_{t}|s_{t}, \theta')$
		\State Receive reward $r_{t}$ and new state $s_{t+1}$
		\State $t\leftarrow t+1$
		\State $T\leftarrow T+1$
		\Until terminal \textbf{or} $t-t_{start}==t_{max}$
		\If {$s_{t}$ is terminal}
		\State $R=0$
		\Else 
		\State $R=V(s_{t},\theta_{v}')$ \textit{//bootstrap from last state }
		\EndIf
		\For {$i \in \left \{ t-1,...,t_{start} \right \}$}
		\State $R\leftarrow r_{i}+\gamma R$
		\State Accumulate gradients wrt $\theta'$:
		\State $d\theta\leftarrow d\theta+\nabla_{\theta'}\textup{log}\pi(a_{i}|s_{i},\theta')(R-V(s_{i},\theta_{v}'))$
		\State Accumulate gradients wrt $\theta_{v}'$:
		\State $d\theta_{v}\leftarrow d\theta_{v} + \partial (R-V(s_{i},\theta_{v}'))^2/ \partial\theta_{v}'$
		\EndFor
		\State Perform asynchronous update of $\theta$ using $d\theta$ and of $\theta_{v}$ using $d\theta_{v}$
		\Until $T>T_{max}$
	\end{algorithmic}
\end{algorithm}

The A3C implementation into the Doom game uses images generated by the vizDoom environment as input data for the well known nonlinear function approximation solution method - ANN. The images become the \textit{states} of the RL problem based on which the AI agent learns to shoot the opponents. The environment vizDoom has a \textit{reward} function predefined, which is triggered when the agent deploys an action in the environment. As the given implementation was made for the \textit{discrete actions space}, the estimated policy or the \textit{actor} provides the probabilities of taking each action in a specific state, and so, in each state the action with the maximal probability is chosen to be pursued. The \textit{critic}, on the other hand, estimates a state-value function for the existing policy and it is used in the composition of the loss function, which represents the performance measure or the \textit{objective function} of the RL problem.

The structure of the ANN is presented in the following figure: