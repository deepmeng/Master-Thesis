\section{Deep Deterministic Policy Gradient (DDPG)}
As mentioned in Cref{sec:DQN} the Deep Q Network solves problems with high-dimensional observation space. But the problem is it can only handle discrete and low-dimensional action space. Many task of interest, most notably physical control tasks, have continuous (real valued) and high dimensional action spaces. The problem with the Deep Q Network is it cannot be applied to continuous domains since it relies on finding the action that maximizes action-value function. In the continuous valued case requires an iterative optimization process at every step. \cite{DBLP:journals/corr/LillicrapHPHETS15}

An obvious approach to adapting the Deep Q Network method to continuous domain is to simply discretize the action space. This have many limitation, most important the curse of dimensionality - the number of actions increases exponentially with the number of degrees of freedom. An example is the human arm is a 7 degrees of freedom system, with a assumption discretization $a_i \sim  \{-k,0,k\}$ for each joint leads to an action space with dimensionality: $3^7 = 2187$. This problem just become bigger with a finer discretion. Such a large action space makes it difficult to explore efficiently. Discretization of action spaces throws away information of the action domain.

The Deep Deterministic Policy Gradient try to solve these problems. The Deep Deterministic Policy Gradient method is a model-free off-policy actor-critic algorithm using deep function approximators that can learn policies in high-dimensional, continuous action space.             