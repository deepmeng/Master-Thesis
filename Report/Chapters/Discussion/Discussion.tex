\chapter{Discussion}\label{Discussion}
The results listed in the previous chapter require some interpretation and discussion. Not all the obtained results look as expected and some aspects need some more thought.

Even though the Adam optimizer is doing more work in it's computations, it has proven to be more efficient in the training than RMSProp. Moreover, the optimal learning rate to be set for the optimizers turned out to be $10^{-4}$, which has been observed to produce the fastest training. This knowledge and parameters were further used in the multi-threading experiments, to gain insight and visualize how much faster the training becomes with more parallel learning workers.

These are all true for the case of the discrete actions, where learning to steer has given the expected results, and which is also true for training more actions, steering and acceleration. Learning more actions, or having one more element in the discrete actions set, hasn't affected the performance that was noticed in training only the steering.

Unfortunately, the continuous actions space didn't provide the expected results. In fact, it never learned to steer, for reasons still uncertain to us. There are some suppositions that were made with respect to the experiment, though. First, the entropy and policy loss calculation differs in the continuous case, therefore the loss function calculation could generate some errors and slacken the learning process. In order to fix the loss function, multiple trials have been deployed, especially minimizing the number of operations and consequently reducing the possibility of error. Second, it was thought that the activation functions for the newly introduced layers in the ANN were wrong. Some more research into the activation functions was conducted and those were also corrected correspondingly. Third, the layer's initialization parameters are very sensible and might affect the whole training, and therefore they were initialized so that they would never be zero. 

It is really difficult to find running examples, debug and troubleshoot ANNs. Especially, because the research in RL is relatively fresh and the area is still in development. The case of continuous actions space didn't have running examples with mean and variance layers. Whereas, there were simplistic implementations without the principles of normal distributions in the DDPG \cite{DDPG_Torcs}, which would be definitely a suggestion to try.

By looking at the result the different reward function gives, it seems that both reward functions is sufficient to learn an agent how to drive a car. The best reward function looks stable enough to learn more than just discrete actions. Because the reward function is important in reinforcement learning, this is an area which cannot be tested enough, and more ideas could be added and tested, to find the best reward function. 

Speed and acceleration has been important in the results of this project. First the method used in this project, was tested with a constant speed, and only having the steering as a trainable parameter. With a constant speed the agent was most stable with a speed of 10 km/h. A solution for getting different speeds to work better, could be to adjust the discrete value for steering, to a specific speed. Because the steering value is optimized to a speed of 10 km/h, and thatâ€™s why a speed of 30 km/h is not stable and a speed of 50 km/h doesn't learn how to drive. 

After trying a new method of using the acceleration as a trainable parameter, it is seen the car learns how to drive with different accelerations as a discrete action. It means the speed is no longer constant. The car no longer only learns how to steer, but also how to accelerate the car by using the throttle. This gives a more realistic view on learning how to drive.  

The results with testing on different track gives the idea, that the agent can learn on every track. But this is not the case, the track used in this project is simple, which makes the agent able to learn, how to drive. The agent has been trained on more complicated tracks, and the agent didn't learn how to drive the car. To make the agent drive on every track, more training and more advance method is needed. In general, the agent will learn how to drive if tested on tracks which looks similar to the track trained on. 
 
\section{Future work}
This project has been a start to implement the A3C method in a car simulation environment. Because of this there are many different experience and improvement, which can be done for future work. Here is mentioned some of the improvement which could be tried.  

Right now, the input image from the environment (TORCS) to the agent is a grayscale image. Instead of a grayscale image a RGB image could be used, as an input. This would give more information to the network, and thereby learn more, and be more flexible. This flexibility could be used for the agent to learn how to drive more complicated tracks.  

In the same category as changing the grayscale image to a RGB image, is changing the image size. The image size could be bigger than 64x64. Right now, 64x64 is used because of the implementation in the TORCS environment, has been easier to implement and use. By making the image size bigger, the network gets more information from the environment, just as changing from grayscale to RGB images. This means, the agent will have more data to train on, and the outcome will be a better more flexible agent.

In the paper of Go \cite{Silver_2016}, before learning the reinforcement learning policy, an initial policy was learned from a human expert with supervised learning. This narrows down the search space and the training that continues, starting with the learned supervised policy, is much faster. The current A3C implementation is learning a soft policy and it is an on-policy algorithm. In the \nameref{Solution Methods} section of chapter 2, it was mentioned the off-policy methods, which result in a more complicated approach, but more stable in the long run. If the off-policy approach with the principle of 2 policies can be combined with the idea used in Go \cite{Silver_2016}, it could result in a better A3C algorithm.

Not long ago, in March 2017, some researchers at Cornell University published an impressive paper about evolution strategies as a scalable alternative to reinforcement learning \cite{EvolStrat}. The paper was also posted on the openai blog. They provided a very good comparison of their results with the Atari paper \cite{DBLP:journals/corr/MnihKSGAWR13} saying that using 720 cores they can "obtain comparable performance to A3C on Atari while cutting down the training time from 1 day to 1 hour" \cite{EvolStrat}. Evolution strategies are known as an optimization method, much easier to implement including for the parallel setting, and it doesn't get affected by scattered rewards. If the resources are available, would be interesting to try the evolution strategies, too.